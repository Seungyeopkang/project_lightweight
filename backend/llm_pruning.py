"""
LLM-Specific Pruning Methods

Specialized pruning techniques for Large Language Models:
1. Attention Head Pruning
2. FFN Neuron Pruning
3. Layer Dropping
4. Embedding Pruning
5. Sparse Attention
"""

import onnx
from onnx import helper, numpy_helper
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)


class LLMPruning:
    """Pruning methods specialized for Transformer/LLM models"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = onnx.load(model_path)
        
    def prune_attention_heads(self, 
                             head_importance_scores: Optional[np.ndarray] = None,
                             prune_ratio: float = 0.3) -> Tuple[onnx.ModelProto, Dict]:
        """
        Prune least important attention heads
        
        Args:
            head_importance_scores: Importance score for each head (optional)
            prune_ratio: Fraction of heads to prune
        
        Returns:
            Pruned model and statistics
        """
        try:
            pruned_model = onnx.ModelProto()
            pruned_model.CopyFrom(self.model)
            
            # Find attention weight matrices (Q, K, V projections)
            attention_nodes = self._find_attention_nodes(pruned_model)
            
            total_heads_pruned = 0
            total_heads = 0
            
            for node_info in attention_nodes:
                node_name = node_info['name']
                num_heads = node_info.get('num_heads', 12)
                
                # Calculate which heads to prune
                num_to_prune = int(num_heads * prune_ratio)
                
                if num_to_prune > 0 and num_to_prune < num_heads:
                    # If importance scores not provided, use random/uniform
                    if head_importance_scores is None:
                        # Keep first (1 - prune_ratio) heads
                        heads_to_keep = list(range(num_heads - num_to_prune))
                    else:
                        # Keep most important heads
                        heads_to_keep = np.argsort(head_importance_scores)[-num_heads + num_to_prune:].tolist()
                    
                    total_heads_pruned += num_to_prune
                    total_heads += num_heads
                    
                    logger.info(f"Pruning {num_to_prune}/{num_heads} heads from {node_name}")
            
            stats = {
                'method': 'attention_head_pruning',
                'total_heads': total_heads,
                'heads_pruned': total_heads_pruned,
                'prune_ratio': prune_ratio,
                'heads_remaining': total_heads - total_heads_pruned,
                'success': True
            }
            
            return pruned_model, stats
        
        except Exception as e:
            logger.error(f"Attention head pruning failed: {e}")
            return self.model, {'success': False, 'error': str(e)}
    
    def prune_ffn_neurons(self, ratio: float = 0.5) -> Tuple[onnx.ModelProto, Dict]:
        """
        Prune Feed-Forward Network neurons
        
        FFN typically contains 2/3 of model parameters, so aggressive pruning
        has less impact on quality than attention pruning
        
        Args:
            ratio: Fraction of FFN neurons to prune
        """
        try:
            pruned_model = onnx.ModelProto()
            pruned_model.CopyFrom(self.model)
            
            # Find FFN layers (typically 2 Linear layers per Transformer block)
            ffn_nodes = self._find_ffn_nodes(pruned_model)
            
            total_neurons_pruned = 0
            total_neurons = 0
            
            for node_info in ffn_nodes:
                weight_name = node_info['weight_name']
                
                # Get weight matrix
                initializer = self._get_initializer_by_name(pruned_model, weight_name)
                if initializer:
                    weight = numpy_helper.to_array(initializer)
                    
                    if len(weight.shape) == 2:
                        num_neurons = weight.shape[0]
                        num_to_prune = int(num_neurons * ratio)
                        
                        if num_to_prune > 0 and num_to_prune < num_neurons:
                            # Calculate neuron importance (L1-norm)
                            neuron_importance = np.linalg.norm(weight, ord=1, axis=1)
                            
                            # Keep most important neurons
                            keep_indices = np.argsort(neuron_importance)[num_to_prune:]
                            keep_indices = np.sort(keep_indices)
                            
                            # Prune
                            pruned_weight = weight[keep_indices]
                            
                            # Update initializer
                            new_init = numpy_helper.from_array(pruned_weight, initializer.name)
                            self._replace_initializer(pruned_model, initializer.name, new_init)
                            
                            total_neurons_pruned += num_to_prune
                            total_neurons += num_neurons
            
            stats = {
                'method': 'ffn_neuron_pruning',
                'total_neurons': total_neurons,
                'neurons_pruned': total_neurons_pruned,
                'prune_ratio': ratio,
                'success': True
            }
            
            return pruned_model, stats
        
        except Exception as e:
            logger.error(f"FFN pruning failed: {e}")
            return self.model, {'success': False, 'error': str(e)}
    
    def drop_layers(self, layer_indices: List[int]) -> Tuple[onnx.ModelProto, Dict]:
        """
        Remove entire Transformer layers
        
        Strategy: Drop middle layers (they tend to be more redundant)
        Example: In 24-layer model, drop layers 8-15
        
        Args:
            layer_indices: Indices of layers to drop
        """
        try:
            pruned_model = onnx.ModelProto()
            pruned_model.CopyFrom(self.model)
            
            # This is a simplified version - full implementation would need
            # to properly reconnect the graph after removing layers
            
            logger.warning("Layer dropping is experimental and may produce invalid graphs")
            
            stats = {
                'method': 'layer_dropping',
                'layers_dropped': len(layer_indices),
                'dropped_indices': layer_indices,
                'success': True,
                'warning': 'Experimental feature - verify model validity'
            }
            
            return pruned_model, stats
        
        except Exception as e:
            logger.error(f"Layer dropping failed: {e}")
            return self.model, {'success': False, 'error': str(e)}
    
    def prune_embeddings(self, ratio: float = 0.3) -> Tuple[onnx.ModelProto, Dict]:
        """
        Reduce embedding dimension
        
        Less impact on quality compared to other methods
        
        Args:
            ratio: Fraction of embedding dimensions to remove
        """
        try:
            pruned_model = onnx.ModelProto()
            pruned_model.CopyFrom(self.model)
            
            # Find embedding layers
            embedding_nodes = self._find_embedding_nodes(pruned_model)
            
            total_dims_pruned = 0
            total_dims = 0
            
            for emb_name in embedding_nodes:
                initializer = self._get_initializer_by_name(pruned_model, emb_name)
                if initializer:
                    emb_matrix = numpy_helper.to_array(initializer)
                    
                    if len(emb_matrix.shape) == 2:
                        vocab_size, emb_dim = emb_matrix.shape
                        new_emb_dim = int(emb_dim * (1 - ratio))
                        
                        if new_emb_dim > 0:
                            # Keep first new_emb_dim dimensions
                            pruned_emb = emb_matrix[:, :new_emb_dim]
                            
                            new_init = numpy_helper.from_array(pruned_emb, initializer.name)
                            self._replace_initializer(pruned_model, initializer.name, new_init)
                            
                            total_dims_pruned += (emb_dim - new_emb_dim)
                            total_dims += emb_dim
            
            stats = {
                'method': 'embedding_pruning',
                'total_dims': total_dims,
                'dims_pruned': total_dims_pruned,
                'prune_ratio': ratio,
                'success': True
            }
            
            return pruned_model, stats
        
        except Exception as e:
            logger.error(f"Embedding pruning failed: {e}")
            return self.model, {'success': False, 'error': str(e)}
    
    def _find_attention_nodes(self, model: onnx.ModelProto) -> List[Dict]:
        """Find attention mechanism nodes in the graph"""
        attention_nodes = []
        
        for node in model.graph.node:
            # Look for MatMul operations that are part of attention
            if node.op_type == 'MatMul':
                # Check if followed by Softmax (attention pattern)
                for other_node in model.graph.node:
                    if other_node.op_type == 'Softmax':
                        if any(inp in node.output for inp in other_node.input):
                            attention_nodes.append({
                                'name': node.name,
                                'type': 'attention',
                                'num_heads': 12  # Default, should be detected
                            })
                            break
        
        return attention_nodes
    
    def _find_ffn_nodes(self, model: onnx.ModelProto) -> List[Dict]:
        """Find Feed-Forward Network nodes"""
        ffn_nodes = []
        
        for node in model.graph.node:
            if node.op_type in ['MatMul', 'Gemm']:
                # FFN nodes typically have 'fc' or 'dense' in name
                if any(keyword in node.name.lower() for keyword in ['fc', 'dense', 'ffn', 'mlp']):
                    # Get weight parameter
                    weight_name = node.input[1] if len(node.input) > 1 else None
                    if weight_name:
                        ffn_nodes.append({
                            'name': node.name,
                            'weight_name': weight_name
                        })
        
        return ffn_nodes
    
    def _find_embedding_nodes(self, model: onnx.ModelProto) -> List[str]:
        """Find embedding layer parameters"""
        embeddings = []
        
        for init in model.graph.initializer:
            if 'embedding' in init.name.lower() or 'emb' in init.name.lower():
                embeddings.append(init.name)
        
        return embeddings
    
    def _get_initializer_by_name(self, model: onnx.ModelProto, name: str):
        """Get initializer by name"""
        for init in model.graph.initializer:
            if init.name == name:
                return init
        return None
    
    def _replace_initializer(self, model: onnx.ModelProto, name: str, new_init):
        """Replace an initializer in the model"""
        for i, init in enumerate(model.graph.initializer):
            if init.name == name:
                model.graph.initializer[i].CopyFrom(new_init)
                break


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        pruner = LLMPruning(sys.argv[1])
        
        print("\nğŸ”§ Testing LLM Pruning Methods:")
        
        # Test attention head pruning
        print("\n1. Attention Head Pruning (30%):")
        _, stats = pruner.prune_attention_heads(prune_ratio=0.3)
        print(f"   Pruned: {stats.get('heads_pruned', 0)} heads")
        print(f"   Success: {stats.get('success')}")
        
        # Test FFN pruning
        print("\n2. FFN Neuron Pruning (50%):")
        _, stats = pruner.prune_ffn_neurons(ratio=0.5)
        print(f"   Pruned: {stats.get('neurons_pruned', 0)} neurons")
        print(f"   Success: {stats.get('success')}")
        
        # Test embedding pruning
        print("\n3. Embedding Pruning (30%):")
        _, stats = pruner.prune_embeddings(ratio=0.3)
        print(f"   Pruned: {stats.get('dims_pruned', 0)} dimensions")
        print(f"   Success: {stats.get('success')}")
