"""
Low-Rank Adaptation (LoRA) for ONNX Models

Decompose weight matrices into low-rank factors:
W (d x k) â‰ˆ A (d x r) @ B (r x k)

Compression ratio: (d*r + r*k) / (d*k)
For r << min(d, k), significant compression with minimal quality loss
"""

import onnx
from onnx import helper, numpy_helper,TensorProto
import numpy as np
from typing import Dict, Tuple, List, Optional
import logging

logger = logging.getLogger(__name__)


def apply_lora_decomposition(model: onnx.ModelProto,
                             rank: int = 8,
                             target_modules: Optional[List[str]] = None) -> Tuple[onnx.ModelProto, Dict]:
    """
    Apply LoRA (Low-Rank Adaptation) to model
    
    Args:
        model: ONNX model
        rank: Rank for decomposition (lower = more compression)
        target_modules: Module name patterns to decompose (e.g., ['query', 'value'])
    
    Returns:
        Decomposed model and statistics
    """
    try:
        lora_model = onnx.ModelProto()
        lora_model.CopyFrom(model)
        
        if target_modules is None:
            # Default: decompose all large weight matrices
            target_modules = ['']  # Empty string matches all
        
        total_params_before = 0
        total_params_after = 0
        decomposed_layers = []
        
        for initializer in lora_model.graph.initializer:
            # Check if this layer should be decomposed
            should_decompose = any(pattern in initializer.name.lower() for pattern in target_modules)
            
            if should_decompose and initializer.data_type == TensorProto.FLOAT:
                weight = numpy_helper.to_array(initializer)
                
                # Only decompose 2D matrices (Linear layers)
                if len(weight.shape) == 2:
                    d, k = weight.shape
                    params_before = d * k
                    total_params_before += params_before
                    
                    # Check if decomposition makes sense
                    if rank < min(d, k):
                        # SVD decomposition
                        A, B = decompose_matrix(weight, rank)
                        
                        params_after = d * rank + rank * k
                        total_params_after += params_after
                        
                        compression = 1 - (params_after / params_before)
                        
                        decomposed_layers.append({
                            'name': initializer.name,
                            'original_shape': (d, k),
                            'rank': rank,
                            'compression': compression,
                            'params_before': params_before,
                            'params_after': params_after
                        })
                        
                        logger.info(f"Decomposed {initializer.name}: {d}x{k} â†’ rank {rank} ({compression:.1%} reduction)")
                        
                        # In full implementation, would replace node with two MatMul nodes
                        # For now, just store the decomposed matrices as metadata
                    else:
                        # Rank too large, skip
                        total_params_after += params_before
                else:
                    # Not a 2D matrix, keep as-is
                    total_params_after += weight.size
        
        overall_compression = 1 - (total_params_after / total_params_before) if total_params_before > 0 else 0
        
        stats = {
            'method': 'lora',
            'rank': rank,
            'total_params_before': total_params_before,
            'total_params_after': total_params_after,
            'compression_ratio': overall_compression,
            'decomposed_layers': len(decomposed_layers),
            'layer_details': decomposed_layers,
            'success': True,
            'note': 'LoRA decomposition computed (graph modification not applied)'
        }
        
        return lora_model, stats
    
    except Exception as e:
        logger.error(f"LoRA decomposition failed: {e}")
        return model, {'success': False, 'error': str(e)}


def decompose_matrix(weight: np.ndarray, rank: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    Decompose weight matrix using SVD
    
    W (d x k) â‰ˆ A (d x r) @ B (r x k)
    
    Args:
        weight: Weight matrix
        rank: Target rank
    
    Returns:
        A, B matrices
    """
    # Perform SVD
    U, S, Vt = np.linalg.svd(weight, full_matrices=False)
    
    # Keep top-r singular values
    A = U[:, :rank] @ np.diag(np.sqrt(S[:rank]))
    B = np.diag(np.sqrt(S[:rank])) @ Vt[:rank, :]
    
    return A, B


def estimate_lora_compression(model: onnx.ModelProto,
                              rank_options: List[int] = [4, 8, 16, 32]) -> Dict:
    """
    Estimate compression for different rank options
    
    Helps user choose optimal rank
    
    Args:
        model: ONNX model
        rank_options: List of ranks to evaluate
    
    Returns:
        Compression estimates for each rank
    """
    results = {}
    
    for rank in rank_options:
        _, stats = apply_lora_decomposition(model, rank=rank)
        results[f'rank_{rank}'] = {
            'compression_ratio': stats.get('compression_ratio', 0),
            'params_after': stats.get('total_params_after', 0),
            'layers_decomposed': stats.get('decomposed_layers', 0)
        }
    
    return {
        'method': 'lora_estimation',
        'rank_options': results,
        'recommendation': _get_lora_recommendation(results)
    }


def _get_lora_recommendation(results: Dict) -> str:
    """Get recommendation for LoRA rank"""
    compressions = {k: v['compression_ratio'] for k, v in results.items()}
    
    # Find rank with good compression but not too aggressive
    if 'rank_8' in compressions and compressions['rank_8'] > 0.3:
        return "Rank 8 recommended: Good balance of compression and quality"
    elif 'rank_16' in compressions:
        return "Rank 16 recommended: Conservative, minimal quality loss"
    else:
        return "Rank 4: Maximum compression (may impact quality)"


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        model = onnx.load(sys.argv[1])
        
        print("\nðŸ”— Testing LoRA Decomposition:")
        
        # Test different ranks
        for rank in [4, 8, 16]:
            print(f"\nRank {rank}:")
            _, stats = apply_lora_decomposition(model, rank=rank)
            
            if stats.get('success'):
                print(f"  Compression: {stats.get('compression_ratio', 0):.1%}")
                print(f"  Params: {stats.get('total_params_before'):,} â†’ {stats.get('total_params_after'):,}")
                print(f"  Layers decomposed: {stats.get('decomposed_layers')}")
            else:
                print(f"  Error: {stats.get('error')}")
        
        # Get recommendation
        print("\nðŸ“Š Compression Estimates:")
        estimates = estimate_lora_compression(model)
        for rank_key, data in estimates['rank_options'].items():
            print(f"  {rank_key}: {data['compression_ratio']:.1%} compression")
        print(f"\nðŸ’¡ {estimates['recommendation']}")
